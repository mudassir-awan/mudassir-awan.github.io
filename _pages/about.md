---
permalink: /
title: "Haptic Researcher and MS-PHD Studen"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<!-- <h2>About me</h2> -->
<!-- <p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 20px;">
  I'm a research assistant at <a href="http://haptics.khu.ac.kr/">Haptics and Virtual Reality Lab</a> and a MS-PHD combined candidate at <a href="https://www.khu.ac.kr/eng/main/index.do">Kyung Hee University</a> in South Korea. I'm advised by <a href="http://haptics.khu.ac.kr/jeon/">Professor Seokhee Jeon</a> and work on data-driven modeling and rendering of haptic properties to generate realistic haptic feedback in VR environments.
</p>

<p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 20px;">
  My research primarily focuses on modeling and rendering of haptic textures using both online and offline approaches. I specialize in handling complex time series data and applying signal processing techniques. I have also utilized these skills in the context of teleoperation systems, where real-time and accurate rendering of haptic textures is crucial for remote operators.
</p>

<p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 20px;">
  Additionally, I explore the development of novel encountered type haptic devices such as haptic drones and wearable haptic devices in VR and AR applications.
</p> -->



<p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 15px; padding-left: 40px; padding-right: 40px;">
  I am a research assistant at <a href="http://haptics.khu.ac.kr/">Haptics and Virtual Reality Lab</a> and a MS-PHD combined student at <a href="https://www.khu.ac.kr/eng/main/index.do">Kyung Hee University</a>. I'm advised by <a href="http://haptics.khu.ac.kr/jeon/">Professor Seokhee Jeon</a> and work on data-driven modeling and rendering of haptic properties to generate realistic haptic feedback in VR environments.
</p>

<p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 15px; padding-left: 40px; padding-right: 40px;">
  My research primarily focuses on modeling and rendering of haptic textures using both online and offline approaches. I specialize in handling complex time series data and applying signal processing techniques. I have also utilized these skills in the context of teleoperation systems, where real-time and accurate rendering of haptic textures is crucial for remote operators.
</p>

<p style="font-size: 14px; text-align: justify; line-height: 1.5; margin-bottom: 15px; padding-left: 40px; padding-right: 40px;">
  Additionally, I explore the development of novel encountered type haptic devices such as haptic drones and wearable haptic devices in VR and AR applications.
</p>



<!-- **<u>News</u>** -->
<h2>News</h2>
<!-- <ul style="margin-bottom: 20px; text-align: justify; font-size: 14px; "> -->
<ul style="margin-bottom: 20px;  font-size: 14px; ">

  <li style="line-height: 1.5;">One paper got accepted at <a href="https://vrst.acm.org/vrst2023/">VRST-2023</a> conference on "Predicting Perceptual Haptic Attributes of Textured Surface from Tactile Data Based on Deep CNN-LSTM Network" (Sept 2023).</li>

  <li style="line-height: 1.5;">One paper got accepted at <a href="https://www.kiise.or.kr/conference/kcc/2023/">KCC-2023</a> conference on "Design and Evaluation of Lightweight Deep Learning Models for Synthesizing Haptic Surface Textures" (June 2023).</li>

  <!-- <li style="line-height: 1.5;"> Join us at <a href="https://2023.ubiquitousrobots.org/">UR-2023</a> conference,  <a href="https://www.meethawaii.com/convention-center/">Hwaaii Convention Center </a>  for our paper presnetaion on "Drone Haptics for 3DOF Force Feedback" on 26th June 2023.</li> -->
  <!-- <li style="line-height: 1.5;">Two papers got accepted in Korea Computer Congress <a href="https://www.kiise.or.kr/conference/kcc/2023/">[KCC-2023] </a> Confernce at Jeju Island, South Korea.</li> -->
  <li style="line-height: 1.5;">I will be presenting our paper on Model Mediated Teleoperation for online Texture modeling and rendering at <a href="https://www.icra2023.org/">ICRA-2023</a> on 1st June, 2023 at Excel London.</li>
  <li style="line-height: 1.5;">One paper got accepted at <a href="https://2023.ubiquitousrobots.org/">UR-2023</a> conference on "Drone Haptics for 3DOF Force Feedback" (April 2023).</li>
  <li style="line-height: 1.5;">One paper got accepted at <a href="https://www.icra2023.org/">ICRA-2023</a> conference titled "Model Mediated Teleoperation for real time haptic texture modeling and rendering" (Feb 2023).</li>
  <!-- <li style="line-height: 1.5;">One paper got accepted at <a href="https://hcikorea.org/">HCI Korea-2023</a> conference on "Haptic Texture Classification using Transformers" (Jan 2023).</li> -->



</ul>



<h2>Selected Publications</h2> 

You can also find the full list of my publications [<span style="color:blue">here</span>](https://mudassir-awan.github.io/publications/)

<table style="width: 100%; border-collapse: collapse; border: 0;">
  <tr>
    <td style="width: 25%; text-align: center; border: none;">
      <img src="/images/VRST.png" alt="Profile Picture" width="160" height="300" style="margin-right: 10px;">
    </td>
    <td style="width: 75%; text-align: justify; border: none;">
      <h3><a href="https://mudassir-awan.github.io/publications/PerceptualAttributes">Predicting Perceptual Haptic Attributes of Textured Surface from Tactile Data Based on Deep CNN-LSTM Network</a>(VRST 2023)</h3>
      <p>
        This paper presents a method to predict human-perceived haptic attributes from tactile signals (acceleration) when a surface is stroked. Using data from 25 texture samples, a five-dimensional haptic space is defined through human feedback, while a physical signal space is created from tool-based interactions. A CNN-LSTM network maps between these spaces. The resulting algorithm, which translates acceleration data to haptic attributes, demonstrated superior performance on unseen textures compared to other models.
      </p>
      <p><a href="https://dl.acm.org/doi/10.1145/3611659.3615714" target="_blank">Full Paper</a> &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Predicting+Perceptual+Haptic+Attributes+of+Textured+Surface+from+Tactile+Data+Based+on+Deep+CNN-LSTM+Network&btnG=" target="_blank">Cite</a></p>
    </td>
  </tr>
</table>

<table style="width: 100%; border-collapse: collapse; border: 0;">
  <tr>
    <td style="width: 25%; text-align: center; border: none;">
      <img src="/images/mmt.png" alt="Profile Picture" width="160" height="300" style="margin-right: 10px;">
    </td>
    <td style="width: 75%; text-align: justify; border: none;">
      <h3><a href="https://mudassir-awan.github.io/publications/teleoperation">Model-Mediated Teleoperation for Remote Haptic Texture Sharing: Initial Study of Online Texture Modeling and Rendering</a>(ICRA 2023 )</h3>
      <p>
        This paper presents the first model-mediated teleoperation (MMT) framework capable of sharing surface haptic texture. It enables the collection of physical signals on the follower side, which are used to build and update a local texture simulation model on the leader side. This approach provides real-time, stable, and accurate feedback of texture. The paper includes an implemented proof-of-concept system that showcases the potential of this approach for remote texture sharing.
      </p>
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10160503" target="_blank">Full Paper</a> &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Model-Mediated+Teleoperation+for+Remote+Haptic+Texture+Sharing%3A+Initial+Study+of+Online+Texture+Modeling+and+Rendering&btnG=" target="_blank">Cite</a></p>
    </td>
  </tr>
</table>


<table style="width: 100%; border-collapse: collapse; border: 0;">
  <tr>
    <td style="width: 25%; text-align: center; border: none;">
      <img src="/images/drone.png" alt="DroneHaptics - Dome-shaped haptic drone for 3-DoF force feedback" width="160" height="300" style="margin-right: 10px;">

    </td>
    <td style="width: 75%; text-align: justify; border: none;">
      <h3><a href="https://mudassir-awan.github.io/publications/haptic-drone">DroneHaptics - Encountered Type Haptic Interface Using Dome-Shaped Drone for 3-DoF Force Feedback</a>(UR 2023)</h3>
      <p>
        This paper introduces a dome-shaped haptic drone with a hemispherical cage made of aluminum mesh. The cage enables controllable 3D force feedback, improving usability and user safety. Experimental measurements and mathematical formulations establish an accurate force-thrust relationship. The system's force rendering accuracy was evaluated, achieving a low error rate of less than 8.6%, ensuring perceptually accurate force feedback.
      </p>
      <p><a href="https://ieeexplore.ieee.org/abstract/document/10202318" target="_blank">Full Paper</a> &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=DroneHaptics+-+Encountered+Type+Haptic+Interface+Using+Dome-Shaped+Drone+for+3-DoF+Force+Feedback&btnG=" target="_blank">Cite</a></p>
    </td>
  </tr>
</table>


<table style="width: 100%; border-collapse: collapse; border: 0;">
  <tr>
    <td style="width: 25%; text-align: center; border: none;">
      <img src="/images/texture_classification_setup.png" alt="Profile Picture" width="160" height="300" style="margin-right: 10px;">
    </td>
    <td style="width: 75%; text-align: justify; border: none;">
      <h3><a href="https://mudassir-awan.github.io/publications/surfaceTexture">Surface Texture Classification Based on Transformer Network</a> (HCI Korea, 2023)</h3>
      <p>
         In this study we developed a new transformer-based deep learning model for surface texture classification from haptic data (i.e., interaction speed, applied force and produced vibration signals). This approach leverages the self-attention process to learn the complex patterns and dynamics of time-series data. To the best of our knowledge this is the first time that the transformer or its variants are used for surface texture classification using tactile information. As a proof of concept, we collected data for 9 different textures and the evaluation experiments showed that the model achieved state-of-the-art classification accuracy.
      </p>
      <p><a href="https://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE11229746" target="_blank">Full Paper</a> &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Surface+Texture+Classification+Based+on+Transformer+Network&btnG=" target="_blank">Cite</a></p>
    </td>
  </tr>
</table>


<table style="width: 100%; border-collapse: collapse; border: 0;">
  <tr>
    <td style="width: 25%; text-align: center; border: none;">
      <img src="/images/Prosthetic_Hand.PNG" alt="Profile Picture" width="160" height="300" style="margin-right: 10px;">
    </td>
    <td style="width: 75%; text-align: justify; border: none;">
      <h3><a href="https://mudassir-awan.github.io/publications/prostheticHand">Vibrotactile Stimulation for 3D Printed Prosthetic Hand</a> (ICRAI, 2016)</h3>
      <p>
         In this study, we address the significant limitation posed by the absence of haptic feedback in prosthetics. Emphasizing the potential of closed-loop prosthetics, our objective is to provide amputees with both exteroceptive (environmental pressure) and proprioceptive (awareness of joint positions) feedback through specialized sensory systems. To this end, we incorporated coin-shaped vibration motors and 0.5-inch circular FSRs to deliver nuanced feedback from the fingertip. For an economical solution, a 3D printed hand was adopted. Our evaluation involved six healthy subjects and one amputee, with the results affirming the efficacy of our approach.
      </p>
      <p><a href="https://ieeexplore.ieee.org/document/7791254" target="_blank">Full Paper</a> &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=DJKWlwoAAAAJ&citation_for_view=DJKWlwoAAAAJ:2osOgNQ5qMEC" target="_blank">Cite</a></p>
    </td>
  </tr>
</table>

